2022/07/18  (10:22)
from: [[Ch-4 Classification]]
to: [[4.2.2 LINEAR DISCRIMINANT ANALYSIS FOR p greater than 1]]

### 4.2.1 LINEAR DISCRIMINANT ANALYSIS FOR p=1:
**p=1 stands for only one predictor**.
Some assumptions:
- $f_{k}$ is [[NORmal distribution]]
$$
f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}e^({\frac{-(x-\mu_k)^2}{2\sigma_k^2}})----(4.15)
$$

- $\sigma_1=....=\sigma_k$ or there is a shared variance.

Therefore, the Bayesian formula here is:
$$
p_k(x)=Pr(Y=k|X=x)= \frac{\pi_{k} *{\frac{1}{\sqrt{2\pi}\sigma_k}e^({\frac{-(x-\mu_k)^2}{2\sigma_k^2}})}}{\sum_{l=1}^{K}\pi_l{\frac{1}{\sqrt{2\pi}\sigma_k}e^({\frac{-(x-\mu_k)^2}{2\sigma_k^2}})}}----(4.16)
$$
This isn't of much use to us, rather we simplify it into:
$$
\delta_k(x)=x*\frac{\mu_k}{\sigma^{2}}-\frac{\mu_k^{2}{2\sigma^{2}}}+log(\pi_k)----(4.17) 
$$
>This is is basically assigning the observation to the class for which $\delta_k(x)$ is largest


>[!note]
> #bayesDecisionBoundary
>the *BAYESIAN DECISION boundary*  is the point / line which we use to classify, for example: 
>if K = 2 and π1 = π2, then the Bayes classifier assigns an observation to class 1 if $2x(µ1 − µ2) > µ^2_1 − µ^2_2$ , and to class 2 otherwise.
>The x value for this point is where $\delta_1=\delta_2$ 
>$$
>x=\frac{\mu_1+\mu_2}{2}
>$$

![[Pasted image 20220714234501.png]]
In real life, classifying something as having a GAUSSIAN or [[NORmal distribution]] for sure is not possible, and we can not use the BAYES classifier.
Hence we do this:
LDA ***LINEAR DISCRIMINANT ANALYSIS***:
$$
\hat{\mu_k}=\frac{1}{n_k}{\sum_{i:y_{i}=k}x_i}
$$
$$
\hat{\sigma_k^2}=\frac{1}{n-K}{\sum\limits_{k=1}^{K}\sum\limits_{i:y_{i}=k}(x_i-\hat\mu_k)^2}--------\text{both}---(4.18)
$$

where:
- n is the total number of training observations
- nk is the number of training observations in the kth class.
- $\hat{\mu_k}$: is the average of the training obervations from the kth class
- $\hat{\sigma_k}^2$: is nothing but the weighted average of the sample variances for each of the K classses.

and the prior class probabilities:
$$
\hat\pi_k=\frac{n_k}{n}----(4.19)
$$

The LDA classifier plugs the estimates given into $\delta_k$, and assigns an observation X = x to the class for which
$$
\hat\delta_k(x)=x*\frac{\hat\mu_k}{\hat\sigma^{2}}-\frac{\hat\mu_k^{2}{2\hat\sigma^{2}}}+log(\hat\pi_k)----(4.20) 
$$
>[!note]
>The word linear in the classifier’s name stems from the fact that the discriminant functions $\hat\delta_k(x)$ in (4.20) are linear functions of x

