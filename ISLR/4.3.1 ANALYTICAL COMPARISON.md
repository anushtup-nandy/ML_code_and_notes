2022/07/21  (11:14)
from: [[Ch-4 Classification]]
to: [[4.3.2 EMPIRICAL COMPARISON]]

### 4.3.1 ANALYTICAL COMPARISON:
Setting with K classes, we generally assign an observation to the class that maximises the LIKELIHOOD FUNCTION. But, in this part, we'll take a look at the logit more due to ease of visualisation.

#### LDA:
$$
log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= log(\frac{\pi_{k}f_k(x)}{\pi_{K}f_K(x)})=a_{k}+\sum\limits_{j=1}^{p} {b_{kj}}{x_j}----(4.24)
$$
where:
$a_k=log(\frac{\pi_k}{\pi_K})-\frac{1}{2}({\mu_{k}+\mu_K})^T{({\sum}^-1)}({\mu_{k}-\mu_K})$
and 
$b_k$ is the jth component of ${({\sum}^-1)}({\mu_{k}-\mu_K})$ .

#### QDA:

$$
log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= log(\frac{\pi_{k}f_k(x)}{\pi_{K}f_K(x)})=a_{k}+\sum\limits_{j=1}^{p} {b_{kj}}{x_j}+{\sum\limits_{j=1}^{p}}{\sum\limits_{l=1}^{p}}{c_{kjl}}{x_j}{x_l}----(4.25)
$$
where $a_k, b_{kj} , \text{ and } c_{kjl} \text{ are functions of } π_k, π_K, µ_k, µ_K, {Σ}_{k}  \text{ and } {Σ}_K$

#### NAIVE BAYES:
$$
log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= log(\frac{\pi_{k}f_k(x)}{\pi_{K}f_K(x)})=a_{k}+\sum\limits_{j=1}^{p} {g_{kj}}{x_j}----(4.26)
$$
$a_k=log(\frac{\pi_k}{\pi_K})$
and 
$g_{kj}(x_j)=log(\frac{f_{kj}}{f_{Kj}})$

#### OBSERVATIONS:
1. LDA is a special case of QDA, with the cjkl=o for all j,l and k
2. Any classifier with linear decision boundary is a special case of NAIVE bayes with $_{kj}=b_{kj}x_j$.
3. If we model the naive bayes with a 1D gaussian distribution, we get a special case of LDA
4. QDA and Naive bayes can never be related:
	1. Naive bayes produces mucch more flexibility, although, it's **purely additive**. Assumes independence of classes.
	2. QDA is more accurate where the interactions are important between the classes.

#### KNN:
Completely **non-parametric approach** (No assumptions about the shape of decision boundary are made!)

Observations:
- dominates over LDA and [[4.1.1 THE LOGISTIC MODEL]] whenever non-linearity is assumed provided n>>>>p
- KNN needs loads of observations compared to the predictors (need to be much lesser).
- KNN does not tell us which predictors are important (does not give us any correlation, std deviation, etc)