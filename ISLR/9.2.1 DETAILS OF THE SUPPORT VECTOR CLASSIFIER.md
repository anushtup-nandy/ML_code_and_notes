2022/08/16  (00:22)
from: [[Ch-9 Support Vector Machines]]
to: [[Ch-9 Support Vector Machines]]

### 9.2.1 DETAILS OF THE SUPPORT VECTOR CLASSIFIER:
The hyperplane is chosen to correctly separate most of the training observations into the two classes, but may misclassify a few observations.
The solution to the following optimization problem with C as a non-negative tuning parameter, M as the width:
![[Pasted image 20220816002323.png]]----(9.4)

We seek to make M as large as possible.

#### $\epsilon's$
The $\epsilon$'s are called *slack variables*--> they allow individual observations to be on the wrong side of the margin/hyperplane. These tell us where the ith obs is located. If $\epsilon_i=0$, then the ith obs is on the rigth side of the margin. If $\epsilon_i>0$, then it's on the wrong side of the margin, whereas, if $\epsilon_i>1$, then it's on the wrong side of the hyperplane.

#### $C$
C bounds the sum of the $\epsilon_i's$, hence it's able to determine the number and severity of the violations to the margin as well as the hyperplane. (Think of C as the budget/amount that something can be violated by the n obs)
As the budget C increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as C decreases, we become less tolerant of violations to the margin and so the margin narrows

In practice, C is treated as a tuning parameter that is generally chosen via cross-validation.

Only observations that either:
- Lie on the margin
- violate the margin
affect the hyperplane.
These are also called the #SupportVectors 

C also controls the [[2.2.2 BIAS-VARIANCE TRADE-OFF]]

>[!important]
>The fact that the support vector classifierâ€™s decision rule is based only on a potentially small subset of the training observations (the support vectors) means that it is quite robust to the behavior of observations that are far away from the hyperplane

>The solution to these equations described in equation (9.4) involves an inner product of observations.
>$$
\text{Inner product of 2 r-vectors, a and b:} \bigl\langle{a,b}\bigr\rangle=\sum\limits_{i=1}^{r}a_ib_i----(9.5)
>$$

The linear support vector classifer can be represented asL
$$
f(x)=\beta_0+\sum\limits_{i=1}^{n}\alpha_i\bigl\langle{x,x_i}\bigr\rangle----(9.6)
$$
where $\alpha$ are n-parameters.
To estimate these paramters,$\alpha, \beta_o$, all we need are the $\frac{n!}{2!(n-2)!}=n(n-2)/2$ inner products between all pairs of training observations.
