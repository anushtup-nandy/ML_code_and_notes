2022/08/10  (11:14)
from: [[5.1.4 BIAS-VARIANCE TRADE-OFF FOR K-FOLD CROSS VALIDATION]]
to: [[Ch-5 Resampling Methods]]

### 5.1.5 CROSS-VALIDATION ON CLASSIFICATION PROBLEMS:
Cross-validation, although mostly used for quantitative analysis, can also perform similarly well on a qualitative set, like in [[Ch-4 Classification]].
We simply use the number of mis-classified observations rather than the MSE.
$$
CV_n=\frac{1}{n}\sum\limits_{i=1}^{n}Err_i----(5.4)
$$

Where $Err_{i}=I(y_{i}\neq \hat{y_i})$.

>[!important]
>Check out [[Bayes theorem]] and [[2.2.4 THE BAYES CLASSIFIER]]

![[Pasted image 20220810113848.png]]
In practice, the #bayesDecisionBoundary and the test error rates are unknown. So, we use cross-validation to make such decisions  (check [[3.3.2 CONFUSION MATRIX]]).
![[Pasted image 20220810114431.png]]

>When using KNN with cross-validation: we see that the training error rate cannot be used to select the optimal value for K. Though the cross-validation error curve slightly underestimates the test error rate, it takes on a minimum very close to the best value for K.





