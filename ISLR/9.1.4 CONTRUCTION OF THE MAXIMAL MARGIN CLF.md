2022/08/15  (23:05)
from: [[9.1.3 THE MAXIMAL MARGIN CLASSIFIER]]
to:[[9.1.5 NON-SEPERABLE CASE]]
### 9.1.4 CONTRUCTION OF THE MAXIMAL MARGIN CLF:
Consider:
- Set of n training observations: $x-1,...,x_{n}\in R^p$
- Class labels: $y_{1},...,y_{n} \in {-1,1}$

The maximal margin hyperplane is the solution to the optimization problem:
$$
\begin{gather*}
\text{maximize}_{\beta_0,....M}M\\
\text{subject to}\sum\limits_{j=1}^{n}\beta_{j}^{2}=1,\text{ and }\:y_i(\beta_0+...+\beta_px_ip){\geq}M\;{\forall}\:i=1,..,n
\end{gather*}----(9.3)
$$
Here:
- the second constraint tells us that each observation will be on the correct side of the hyperplane, given that M is positive.
- the first constraint tells us about the perpendicular distance of the line from he ith observation.
