2022/07/18  (10:23)
from: [[4.2.1 LINEAR DISCRIMINANT ANALYSIS FOR p=1]]
to: [[4.2.3 QUADRATIC DICRIMINANT ANALYSIS]]

### 4.2.2 LINEAR DISCRIMINANT ANALYSIS FOR p>1:
$$
X=(X_1,........X_p)
$$
We need to assume this, and that the predictors are drawn from a [[Multivariate Gaussian distribution]], with *class specific mean vector* and a *common [[Covariance Matrix]]*.
Simple Gaussian multivariate matrix:
$$
p(x|\mu,\sum\limits)=\frac{1}{2\pi^{\frac{p}{2}}}|\sum\limits|^{\frac{-1}{2}}exp[{\frac{-(x-\mu)(\sum)^{-1}(x-\mu)^T}{2}}]
$$
![[Pasted image 20220716214439.png]]
>[!note]
>there are 3 lines because there are 3 pairs of classes possible among the three classes.

>[!important]
>LDA decision boundaries are always close to the Bayes decision boundaries.

In practice, a <u>binary classifier such as this one</u> can make two types of errors: **it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to the default category.**----> ==CONFUSION MATRIX helps us do exactly that!==

We also take into consideration: **sensitivity(true positivity rate or *recall*) and specificity**. 

>[!important]
>As total error rate reduces, the recall also reduces.

We can use something called the ***THRESHOLD*** to improve our LDA analysis.
![[Pasted image 20220717004532.png]]

#### ROC curve:
![[Pasted image 20220717004304.png]]
ROC curve is a popular method for simultaneously displaying the 2types of errors of all possible thresholds.
Some metrics of the ROC curve:
- overall performance of classifier--> AUC (area under the curve)
- Ideal ROC hugs the left corner
- ROC is useful for comparing different classifiers. (they take into account all possible thresholds)
- ROC is usuall btw--> tpr vs fpr

