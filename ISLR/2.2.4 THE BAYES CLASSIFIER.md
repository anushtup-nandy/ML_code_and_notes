
from: [[2.2.3 CLASSIFICATION SETTING]]
to: [[Ch-2 Statistical learning]]
### 2.2.4 THE BAYES CLASSIFIER:
That the test error rate given in (2.9) is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values
Simply assign a test observation with predictor vector x0 to the class j for which 
$$Pr(Y = j|X = x0) ----(2.10)$$is largest.
>[!tip]
>Remember Conditional probability

The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate.
$$
1 - E (max_j Pr(Y = j|X) )----(2.11)
$$
#### 2.2.4.1 K-NEAREST NEIGHBORS: 
the Bayes classifier serves as an unattainable gold standard against which to compare other methods.
One such method is the K-nearest neighbors (KNN) classifier.
Given a positive in K-nearest neighbors integer K and a test observation x0, the KNN classifier first identifies the K points in the training data that are closest to x0, represented by N0. It then estimates the conditional probability for class j as the fraction of points in N0 whose response values equal j:
$$
Pr(Y=j|X=x_{0})= \frac{1}{k}\sum_{i \epsilon N_{0}}I(y_i=j)----(2.12)
$$
The choice of K has a drastic effect on the KNN classifier obtained.

>[!Important]
>As 1/K increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10) before increasing again when the method becomes excessively flexible and overfits.

![[Pasted image 20220712171907.png]]
