from:[[Ch-2 Statistical learning]]
to:[[2.2.2 BIAS-VARIANCE TRADE-OFF]]
### 2.2.1 MEASURING THE QUALIT OF FIT:
In the regression setting, the most commonly-used measure is the mean squared error (MSE)
$$
MSE=\frac{1}{n}\sum_{i=1}^{n} (y_{i}- \hat{f}(x_{i))^2}----(2.5)
$$
In general, we do not care in the accuracy of the method on the training data, rather, we wan't to get it working on the test set(previously unseen data).

Suppose that we fit our statistical learning method on our training observations **{(x1, y1),(x2, y2), . . . ,(xn, yn)}**, and we obtain the estimate $\hat{f}$. We can then compute **ˆf(x1), ˆf(x2), . . . , ˆf(xn)**. If these are approximately equal to y1, y2, . . . , yn, then the training MSE given by (2.5) is small. 
However, we are really not interested in whether $\hat{f}(x_i) ≈ y_i$ ; *instead, we want to know whether **ˆf(x0)** is approximately equal to **y0, where (x0, y0)* is a previously unseen test observation** not used to train the statistical learning method. Hence, we compute:
$$
Ave(y_{0}-\hat{f}(x_0))----(2.6)
$$

![[Pasted image 20220712154725.png]]
The grey curve displays the average training MSE as a function of ***flexibility***, or more formally the ==degrees of freedom==, for a number of smoothing spline

>[!important]
>As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. ( #overfitting )

Minimal test MSE can be found using something like : ==CROSS-VALIDATION==