2022/07/12  (14:12)
Tag:   #ml #study/islr [[MOC_study]]

# Ch-2 Statistical learning
## 2.1. What is statistical learning?
We observe a quantitative response Y and p different predictors, X1, X2, . . . , Xp. We assume that there is some relationship between Y and X = (X1, X2, . . . , Xp), which can be written in the very general form:
$$
Y = f(X) + \epsilon----(2.1)
$$
In essence, statistical learning refers to a set of approaches for estimating f. In this chapter we outline some of the key theoretical concepts that arise in estimating f, as well as tools for evaluating the estimates obtained.
- [[2.1.1WHY ESTIMATE f]]
- [[2.1.2 HOW DO WE ESTIMATE f]]
- [[2.1.3 TRADE-OFF BETWEEN PREDICTION ACC. AD INTERPRETABILITY]]
- [[2.1.4 SUPERVISED VS UNSUPERVISED]]
- [[2.1.5 REGRESSION VS CLASSIFICATION]]

## 2.2 Asserting Model Accuracy:
There is ***no free lunch in statistics***: no one method dominates all others over all possible data sets.

- [[2.2.1 MEASURING THE QUALIT OF FIT]]
- [[2.2.2 BIAS-VARIANCE TRADE-OFF]]
- [[2.2.3 CLASSIFICATION SETTING]]
- [[2.2.4 THE BAYES CLASSIFIER]]

---
# References
[An Introduction To Statistical Learning With Applications In R (Second Edition) (Gareth James, Daniela Witten, Trevor Hastie etc.) (z-lib.org).pdf](file:///C:/Users/Anushtup%20Nandy/OneDrive/Documents/BITS/important%20stuff/EXTRA%20BOOKS/Machine%20Learning/An%20Introduction%20To%20Statistical%20Learning%20With%20Applications%20In%20R%20(Second%20Edition)%20(Gareth%20James,%20Daniela%20Witten,%20Trevor%20Hastie%20etc.)%20(z-lib.org).pdf)