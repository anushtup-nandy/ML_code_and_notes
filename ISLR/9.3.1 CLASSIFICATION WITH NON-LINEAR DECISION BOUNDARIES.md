2022/08/17  (18:18)
from: [[Ch-9 Support Vector Machines]]
to: [[9.3.2 THE SUPPORT VECTOR MACHINE]]

### 9.3.1 CLASSIFICATION WITH NON-LINEAR DECISION BOUNDARIES:

Support vector classifier(9.2) is very useful in the **2 class** setting. (i.e., when we need to seperate 2 classes)--> iff the boundary between them is linear!.

If the boundary is non-linear, we tend to look for other methods-> we generally tend to expand the feature space:
- done using functions of predictors like using a quadratic or cubic terms.
Suppose we assume a setting with 2p features instead of p like: $X_1,X_1^2,...,X_{p}^2$
equations (9.4) become something like:
![[Pasted image 20220817180640.png]]
Here, the enlarged decision boundary is linear (basically after the expansion!). But the original one, $q(x)=0$ was one with quadratic terms.

We generally like to expand the features with modifications like:
- higher order polynomial terms
- interaction of the form of:
	- $X_jX_{j'}$ where $j\neq j'$

The inner product that we showed in [[9.2.1 DETAILS OF THE SUPPORT VECTOR CLASSIFIER]], can be written in a more generalised form like:

$$
K(x_i,x_{i'})=Kernel
$$

A kernel is just a function that quanitfies the similarity of 2 observations.
Linear kernel:

$$
K(x_i,x_{i'})=\sum\limits_{i=1}^p{x_{ij}x_{i'j}}
$$

Polynomial kernel:

$$
K(x_i,x_{i'})=(1+\sum\limits_{i=1}^p{x_{ij}x_{i'j}})^{d}{\:\:\:\:}\forall{\:d\in{I^+}}----(9.7)
$$

Radial kernel:

$$
K(x_i,x_{i'})=exp(-\gamma\sum\limits_{j=1}^{p}(x_{ij}-x_{i'j})^2)----(9.8)

$$
Often we'll be using the RBF kernel ([[NORmal distribution]])
- The radial kernel works in a very *local* manner, only obs very close to it actually have an effect on it.
	- since the term inside exp will be large so the entire term will be small!

When the support vector classifier is combined with some sort of non-linear kernel, we get the **SUPPORT VECTOR MACHINE**.

$$
f(x)=\beta_0+\sum\limits_{i=1}^{n}\alpha_iK(x_i,x_{i'})----(9.9)
$$
