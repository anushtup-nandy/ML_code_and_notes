2022/08/14  (12:15)
from: [[9.1.1 WHAT'S A HYPERPLANE]]
to:[[9.1.3 THE MAXIMAL MARGIN CLASSIFIER]]

### 9.1.2 CLASSIFICATION USING A SEPERATING HYPERPLANE:
Suppose that we have a nxp data matrix $\bf{X}$ that consists of $n$ training obs in a p-dimensional space.
$$
x_1=\begin{pmatrix}x_{11}\\x_{12}\\.\\.\\x_{1p}\end{pmatrix},...,x_n=\begin{pmatrix}x_{n1}\\x_{n2}\\.\\.\\x_{np}\end{pmatrix}----(9.2)
$$
These observations fall into 2 classes: $y_1,...y_n\in{-1,1}$.
We can use a simple hyperplane like:
$$
\beta_0+\beta_{1x_{i1}}+....+\beta_{1x_{ip}}\geq or \leq 0
$$
it'll be $\geq0$ if $y_i=1$, and $\leq0$ if $y_i=-1$.

>If a separating hyperplane exists, we can use it to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located.

We can use this to create a function for a test obs dataset like:
$$
f(x^*)=\beta_0+\beta_1{x_1^*}+....\beta_p{x_p^*}
$$
if this is positive, the  we assign to class-1, otherwise 2.

>[!warning]
>This is very similar to [[Ch-3 Linear Regression]]

![[Pasted image 20220814125734.png]]
