2022/08/06  (17:15)
from:[[3.3.2 EXTENSIONS OF THE LINEAR MODEL]]
to: [[CH-3 CLASSIFICATION]]

### 3.3.3 POTENTIAL PROBLEMS:
These are also some sort of assumptions.
1. Non-linearity of response-predictor relationships
2. Correlation of error terms
3. Non-constant variance of error terms
4. outliers
5. high-leverage points
6. Collinearity

#### Non-linearity of the data:
Linear regression assumes the "straight-line" relationship. This is often risky, so we use the ***RESIDUAL PLOTS*** . These are useful for identifying non-linearity. ( #ResidualPlots )

>[!note]
>Residual plots::$e_i=y_i-\hat{y_i}$  vs $x_i$. 

	Given a simple linear regression we can plot the residuals .
	In case of a multiple regression we plot the residuals vs the predicted(fitted) values

In case a residual plot indicates non-linear relationships, we can use non-linear transformations. (like$logX,\sqrt{X},etc$)

#### Correlation of error terms:
If there's correlation among the error terms, then the *Standard error* that we calculated would be lesser than the *true standard error.*

Example:
- Supppose we doubled our data accidently--> would lead to error terms identical in pairs. If we ignore this then we treat it as a sample of size 2n, whereas it's a sample of sie n only but the [[CONFIDENCE INTERVALS]] would be narrower by $\sqrt{2}$.

If there is positive correlation, then there wll be something known as **TRACKING**- adjacent residuals may have very similar values.

![[Pasted image 20220806180226.png]]
(They have a correlation of 0.9)

>This assumption is very important to be explored in a lot of models due to it's ability to mitigate the risk.

#### Non-constant variance of error terms:
The standard errors, [[CONFIDENCE INTERVALS]] and [[HYPOTHESIS TESTS]] associated with the linear model rely on this assumption.
One can find the non-constant errors or the *HETEROSCEDASTICITY* from the presence of a *funnel shape* in the #ResidualPlots .

>When faced with such aproblem, a solution can be to *transform* the response Y using a concave function like $logY, \text{ or }\sqrt{Y}$.

![[Pasted image 20220806181930.png]]

If we have a good idea of the variance of each response. Like if all the observations are uncorrelated with some variance $\sigma^2$, then their average has the variance, $\sigma_i^2=\sigma^2/n_i$. In such situations, it's possible to fit our model using *WEIGHTED LEAST SQUARES*, with the weights being proportional to the inverse of the variances.

#### Outliers:
Outliers are points for which the $y_i$ is far from the value predicted by the model.
They can cause lots of errors:
- it may cause the RSE to inc or dec, and hence affect the p-values!
- it increases or dec the $R^2$ values (depends on the position of the outlier wrt the line)

Residual plots can be used to identify outliers.

To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual, $e_i$ by it's estimated standard error.

#### High Leverage Points:
![[Pasted image 20220806183708.png]]
41-high leverage
20-outlier

High leverage are basically points with very unusual values for $x_i$. High leverage points have hige impacts on the estimated regression lines.
In a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors. SUCH A point is shown in the centre panel of the figure 3.13.

To find these we compute the [[leverage statistic]]:
$$
h_i=\frac{1}{n}+\frac{(x_i-\bar{X})^2}{\sum\limits_{i'=1}^{n}(x_i-\bar{x})^2}----(3.17)
$$
a high value of this indicates an observation with high leverage.

#### Collinearity:
Situation where 2 or more predictor variables are closely related to one another.
We can compute scatter-plots to find collinearity.![[Pasted image 20220806184452.png]]

We can also compute Contour plots.
![[Pasted image 20220806184636.png]]
Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\hat\beta_j$ to grow. Recall that the t-statistic for each predictor is calculated by dividing $\hat\beta_j$ by its standard error. Consequently, collinearity results in a decline in the t-statistic.
As a result, in the presence of collinearity, we may fail to reject H0 : βj = 0. This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.

##### solution:
A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data

Not all can be found, it is possible for collinearity to be found even if no pair of variables has a particularly high correlation.--> *MULTICOLLINEARITY* 

***VARIANCE INFLATION FACTOR (VIF)*** tells us a better picture than the correlation matrix.

>As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.


$$
VIF(\hat\beta_j)=\frac{1}{1-R^2_{X_j|X_{-j}}}----(3.18)
$$
the $R^2_{X_j|X_{-J}}=R^2$ of the $X_j$ onto all other predictors.

When faced with the problem of collinearity-->
1. Drop off one of the problematic variables
	1. Can be done without much effect to the dataset due to the fact that the particular variable provides *redundant information* about the response.
2. We can combine the 2 collinear variables.
	1. Best way is to take their *standardised versions* and create a new one.

