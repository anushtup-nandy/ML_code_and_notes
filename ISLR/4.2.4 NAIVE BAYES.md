2022/07/18  (11:13)
from: [[4.2.3 QUADRATIC DICRIMINANT ANALYSIS]]
to: [[Ch-4 Classification]]

### 4.2.4 NAIVE BAYES:
In LDA and QDA we made assumptions about the ==$f_k(x)$ (which is a p-dimentional density function, where p is the number of predictors.)==
LDA: $f_k$ is a PDF of [[Multivariate Gaussian distribution]], with constant $\mu, \text{and shared }{\sum}_k$.
QDA: $f_k$ is a PDF of [[Multivariate Gaussian distribution]], with class-specific $\mu, {\sum}_k$.

Naive Bayes: (VERY DIFFERENT) we assume the following:
>Within the Kth cclass, the p predictors are independent.

$$
f_k(x)=f_{k1}(x_1)*f_{k2}(x_2)*f_{k3}(x_3)...........*f_{kp}(x_p)----(4.22)
$$
>[!important]
>Why is this assumption so powerful? Essentially, estimating a p-dimensional density function is challenging because we must consider not only the marginal distribution of each predictor — that is, the distribution of  each predictor on its own — but also the joint distribution of the predictors joint distribution — that is, the association between the different predictors. But, now we can stop worying about the association because there is none!

Naive Bayes method :
1. increases bias
2. reduces the variance
>**Therefore, it should be used  when  n is not that large compared to p**.

Final estimate we need to make:
$$
Pr(Y=k|X=x)=\frac{{\pi_k}*{f_{k1}(x_1)*f_{k2}(x_2)*f_{k3}(x_3)...........*f_{kp}(x_p)}}{\sum\limits_{l=1}^{K}{f_{l1}(x_1)*f_{l2}(x_2)*f_{l3}(x_3)...........*f_{lp}(x_p)}}----(4.23)
$$

To estimate the 1D density function $f_{kj}$ using the training data ($x_{1j},.....,x_{nj}$):
- if $X_j$ is quantitative we can assume that $X_j|Y=k{\sim} N(\mu_{jk}, \sigma_{jk}^2)$ .
	- Non-parametric (check [[2.1.2 HOW DO WE ESTIMATE f]]) estimate using histograms of the jth predictor (within each class), after which we can simply estimate $f_{kj}(x_j)$ as a fraction of the training set in the kth class.
- if $X_j$ is quanlitative we can assume that:
	- 	- We can simply count the proportion of training set observations for jth predictor corresponding to each class.

>[!note] 
>While LDA has a slightly lower overall error rate, naive Bayes correctly predicts a higher fraction of the true defaulters.
>We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where p is larger or n is smaller, so that reducing the variance is very important

