2022/07/18  (10:20)
from: [[4.1.4 MULTIPLE LOGISTIC REGRESSION]]
to: [[Ch-4 Classification]]
### 4.1.5 MULTINOMIAL LOGISTIC REGRESSION:

	We use this when we need to classify a response variable that has more than 2 classes

To do this we have to select a single class to serve as the *baseline*. In general we end up selecting the **Kth class** for this role.
$$
Pr(Y=k|X=x)= \frac{e^{\beta_{k0}+\beta_{k1}x_1+....+\beta_{kp}x_p}}{1+\sum_{l=1}^{k-1} e^{\beta_{l0}+\beta_{l1}x_1+....+\beta_{lp}x_p}}----(4.10)
$$

for k=1,....K-1.
Therefore, taking the ==baseline== that we just chose:
$$
Pr(Y=K|X=x)= \frac{1}{1+\sum_{l=1}^{k-1} e^{\beta_{l0}+\beta_{l1}x_1+....+\beta_{lp}x_p}}----(4.11)
$$

>[!note]
>See that 4.10/4.11 is the ODDS!!

==LOGIT==
$$
log (\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})= \beta_{k0}+\beta_{k1}x_1+....+\beta_{kp}x_p----(4.12)
$$
Here on we must take care to interpret the coefficients, since they'll change everytime we change the baseline!

>[!important]
>*SOFTMAX coding*: Means that regardless of the coding, the fitted values, log odds between any pair of classes, and other key outputs will remain the same. 
>It is an alternative to multinomial logistic regression.

#softmax
In softmax coding, we usually treat all the K classes symmetrically. This means that estimating K-1 classes is not an issue any more, we can directly estimate the coeffients of K classes!!
$$
Pr(Y=k|X=x)= \frac{e^{\beta_{k0}+\beta_{k1}x_1+....+\beta_{kp}x_p}}{\sum_{l=1}^{K} e^{\beta_{l0}+\beta_{l1}x_1+....+\beta_{lp}x_p}}
$$
And we find the ==LOGIT== for Kth and K'th classes.
$$
log (\frac{Pr(Y=k|X=x)}{Pr(Y=k'|X=x)})= (\beta_{k0}-\beta_{k'0})+(\beta_{k1}-\beta_{k'1})x_1+....+(\beta_{kp}-\beta_{k'p})x_p----(4.13)
$$

>[!warning]
>**Multinomial regression** : one dependent variable(more than two categories for logistic regression) and more than one independent variable. 
>**Multivariate regression** : It's a regression approach of more than one dependent variable.

