2022/08/10  (08:33)
from: [[5.1.1 THE VALIDATION SET APPROACH]]
to: [[5.1.3 K-FOLD CROSS-VALIDATION]]

### 5.1.2 LEAVE-ONE-OUT CROSS-VALIDATION:
LOOCV splits the dataset into 2 parts. *It doesn't create 2 sets of comparable size*. Instead it uses a single observation for validation set, and the remaining ones make up the training set.

> Learning methods are fit on n-1 training observations and a prediction $\hat y_1$ is made on the excluded one.

This procedure is repeated for all the other n-1 observations

So, the LOOCV estimate for the MSE is the average of the n-test error estimates:
$$
CV_{(n)}=\frac{1}{n}\sum\limits_{i=1}^{n}MSE_i----(5.1)
$$


#### ADVANTAGES:
1. Far less bias
	1. Due to the repeated fitting
	2. Less overestimating the test error rate
2. Less randomness.
	1. Validation error approach gives us different results if performed on different sub-sets of the same dataset
	2. LOOCV doesn't do that.

![[Pasted image 20220810084638.png]]
#### DISADVANTAGES:
Can be very slow, because we're implementing it n times.

There's a short-cut tho:
$$
CV_n=\frac{1}{n}\sum\limits_{i=1}^{n}(\frac{y_{i}-\hat{y_i}}{1-h_i})^2----(5.2)
$$
Here:
- $h_i$ is the leverage defined in [[3.3.3 POTENTIAL PROBLEMS]]. ([[leverage statistic]])
With least sqaures or polynomial regression, this makes the cost of LOOCV the same as single model fit!

![[Pasted image 20220810103153.png]]



